{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch:\t2.0.0+cu117\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import gym\n",
    "import sys\n",
    "\n",
    "import torch\n",
    "from torch import nn \n",
    "from torch import optim\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "print(\"PyTorch:\\t{}\".format(torch.__version__))\n",
    "\n",
    "class policy_estimator():\n",
    "    def __init__(self, env):\n",
    "        self.n_input = env.observation_space.shape[0]\n",
    "        self.n_outputs = env.action_space.n\n",
    "        \n",
    "        #Define network\n",
    "        self.network = nn.Sequential(\n",
    "            nn.Linear(self.n_input, 16),\n",
    "            nn.ReLU(), \n",
    "            nn.Linear(16, self.n_outputs),\n",
    "            nn.Softmax(dim=-1)\n",
    "        )\n",
    "        \n",
    "    def predict(self, state):\n",
    "        action_probs = self.network(torch.FloatTensor(state))\n",
    "        return action_probs\n",
    "                            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "def discount_rewards(rewards, gamma=0.99):\n",
    "            r = np.array([gamma**i * rewards[i] for i in range(len(rewards))])\n",
    "            #Reverse the array direction for cumsum and then revert back to the original order\n",
    "            r = r[::-1].cumsum()[::-1]\n",
    "            return r - r.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reinforce(env, policy_estimator, num_episodes=2000,\n",
    "                      batch_size=10, gamma=0.99):\n",
    "    #Set up lists to hold results\n",
    "    total_rewards = []\n",
    "    batch_rewards = []\n",
    "    batch_actions = []\n",
    "    batch_states = []\n",
    "    batch_counter = 1\n",
    "            \n",
    "    # Define optimlizer     \n",
    "    optimizer = optim.Adam(policy_estimator.network.parameters(), lr=0.01)\n",
    "            \n",
    "    action_space = np.arange(env.action_space.n)\n",
    "    ep = 0\n",
    "    while ep < num_episodes:\n",
    "        s_0 = env.reset()\n",
    "        states = {agent_id: list() for agent_id in s_0.keys()}     \n",
    "        rewards = {agent_id: list() for agent_id in s_0.keys()}\n",
    "        actions = {agent_id: list() for agent_id in s_0.keys()}\n",
    "        done = False\n",
    "                \n",
    "        while not done:\n",
    "            #Get action and convert it to joint_action\n",
    "            joint_action = {agent_id: {'movement':None, 'ask_communication': 1} for agent_id in s_0.keys()}\n",
    "            for agent_id in s_0.keys():\n",
    "                action_probs = policy_estimator.predict(s_0[agent_id]).detach().numpy()\n",
    "                joint_action[agent_id]['movement'] = np.random.choice(action_space, p=action_probs)\n",
    "                    \n",
    "                s_1, r_0, dones, _ = env.step(joint_action)\n",
    "\n",
    "                \n",
    "                {k:v.append(s_0[k]) for k,v in states.items()}\n",
    "                {k:v.append(r_0[k]) for k,v in rewards.items()}\n",
    "                {k:v.append(joint_action[k]['movement']) for k,v in actions.items()}\n",
    "                s_0 = s_1\n",
    "                    \n",
    "                done = False not in dones.values()\n",
    "                # If done, batch data\n",
    "                if done:\n",
    "                    for agent_id in s_0.keys():\n",
    "                        if batch_counter < batch_size:\n",
    "                            batch_rewards.extend(discount_rewards(rewards[agent_id], gamma))\n",
    "                            batch_states.extend(states[agent_id])\n",
    "                            batch_actions.extend(actions[agent_id])\n",
    "                            batch_counter += 1\n",
    "                                \n",
    "                            total_rewards.append(sum(rewards[agent_id]))\n",
    "                            \n",
    "                    #If batch is complete, update network\n",
    "                    if batch_counter == batch_size:\n",
    "                        optimizer.zero_grad()\n",
    "                        state_tensor = torch.FloatTensor(batch_states)\n",
    "                        reward_tensor = torch.FloatTensor(batch_rewards)\n",
    "                        # Actions are used as indices, must be LongTensor\n",
    "                        action_tensor = torch.LongTensor(batch_actions)\n",
    "                            \n",
    "                        #Calculate loss\n",
    "                        logprob = torch.log(policy_estimator.predict(state_tensor))\n",
    "                        selected_logprobs = reward_tensor * torch.gather(logprob, 1, action_tensor.unsqueeze(1)).squeeze()\n",
    "                        loss = -selected_logprobs.mean()\n",
    "                            \n",
    "                        # Calculate gradients\n",
    "                        loss.backward()\n",
    "                        # applu gradients\n",
    "                        optimizer.step()\n",
    "                            \n",
    "                        batch_rewards = []\n",
    "                        batch_actions = []\n",
    "                        batch_states = []\n",
    "                        batch_counter = 1\n",
    "                            \n",
    "                        avg_rewards = np.mean(total_rewards[-100:])\n",
    "                        # Print running average\n",
    "                        print(\"\\rEp: {} Average of last 100:\" +   \n",
    "                            \"{:.2f}\".format(\n",
    "                            ep + 1, avg_rewards), end=\"\")\n",
    "                        ep += 1\n",
    "    return total_rewards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ep: {} Average of last 100:2000.00"
     ]
    }
   ],
   "source": [
    "env = gym.make('CartPole-v0')\n",
    "policy_est = policy_estimator(env)\n",
    "rewards = reinforce(env, policy_est)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn import Module\n",
    "from torch.nn import Conv2d\n",
    "from torch.nn import Linear\n",
    "from torch.nn import MaxPool2d\n",
    "from torch.nn import ReLU\n",
    "from torch.nn import LogSoftmax\n",
    "from torch import flatten\n",
    "\n",
    "class VGGBLOCK(Module):\n",
    "    def __init_(self, numChannels, classes):\n",
    "        super(VGGBLOCK,self).__init__()\n",
    "        \n",
    "        self.conv1 = Conv2d(in_channels=6, out_channels=128, kernel_size = (3,3), stride=1, padding=1)\n",
    "        self.conv2 = Conv2d(in_channels=128, out_channels=128, kernel_size = (3,3), stride=1, padding=1)\n",
    "        self.conv3 = Conv2d(in_channels=128, out_channels=128, kernel_size = (3,3), stride=1, padding=1)\n",
    "        self.pool1 = MaxPool2d(size=2, stride=1)\n",
    "    \n",
    "    def forwar\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0.2451, -0.2843, -0.3353,  0.0593,  0.3627],\n",
       "         [-0.4267,  0.0633, -0.6618,  0.3487, -0.1211],\n",
       "         [ 0.0750, -0.4737,  0.3125,  0.7875, -0.3587],\n",
       "         [ 0.8480,  0.0339,  0.4692,  0.0946, -0.4433],\n",
       "         [ 0.1824, -0.0720,  0.0922, -0.2039,  0.6299]]],\n",
       "       grad_fn=<SqueezeBackward1>)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "m = nn.Conv2d(in_channels=6, out_channels=1, kernel_size=3, stride=1,padding=1)\n",
    "input = torch.randn(6,5,5)\n",
    "m(input)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
